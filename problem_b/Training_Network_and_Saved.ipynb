{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pickle\n",
    "# for plot and stimulate\n",
    "def simulate_squid_game_v2(L, N, p, T):\n",
    "    \"\"\"\n",
    "    Simulate the Squid Game scenario for T turns.\n",
    "    \n",
    "    Parameters:\n",
    "    - L (int): Max number of players that can be sent back to the start line\n",
    "    - N (int): Total number of players\n",
    "    - p (list): List of N probabilities of failing to freeze for each player\n",
    "    - T (int): Number of turns to simulate\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary where the keys are player indices and the values are dictionaries.\n",
    "          The inner dictionaries have keys as positions and values as the probability of \n",
    "          each player being in that position after T turns.\n",
    "    \"\"\"\n",
    "    # Initialize players' positions\n",
    "    positions = np.zeros(N, dtype=int)\n",
    "    \n",
    "    # Store the history of each player's position\n",
    "    history = np.zeros((N, T+1), dtype=int)\n",
    "    \n",
    "    # Simulate the game for T turns\n",
    "    for t in range(T):\n",
    "        # Move all players one step forward\n",
    "        positions += 1\n",
    "        \n",
    "        # Determine which players failed to freeze\n",
    "        failed_to_freeze = np.random.rand(N) < p\n",
    "        \n",
    "        # If more than L players failed to freeze, find the L players who traveled the farthest\n",
    "        if np.sum(failed_to_freeze) > L:\n",
    "            # Find the indices of the L players who traveled the farthest and failed to freeze\n",
    "            farthest_failed_indices = np.argsort(positions * failed_to_freeze)[-L:]\n",
    "            \n",
    "            # Send them back to the starting line\n",
    "            positions[farthest_failed_indices] = 0\n",
    "        else:\n",
    "            # Send all players who failed to freeze back to the starting line\n",
    "            positions[failed_to_freeze] = 0\n",
    "        \n",
    "        # Update history\n",
    "        history[:, t+1] = positions\n",
    "        \n",
    "    # Calculate the probability distributions for each player\n",
    "    prob_distributions = {}\n",
    "    for i in range(N):\n",
    "        # Count occurrences of each position and divide by T to get probabilities\n",
    "        unique, counts = np.unique(history[i, :], return_counts=True)\n",
    "        prob_distributions[i] = dict(zip(unique, counts))\n",
    "    \n",
    "    return prob_distributions\n",
    "\n",
    "def plot_simulation_vs_estimate(p, result, probabilities_estimated, people_order=0,T=1000000):\n",
    "    \"\"\"\n",
    "    Plot the simulated and estimated probabilities along with their relative error.\n",
    "    \n",
    "    Parameters:\n",
    "    - result (dict): Output from the simulate_squid_game_v2 function\n",
    "    - p (list): List of N probabilities of failing to freeze for each player\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Extract position probabilities for player 0 from simulation\n",
    "    #probabilities_estimated*=T\n",
    "    positions_simulated = list(result[people_order].keys())\n",
    "    probabilities_simulated = list(result[people_order].values())\n",
    "    probabilities_simulated = np.array(probabilities_simulated)/sum(probabilities_simulated)\n",
    "    \n",
    "\n",
    "    # Estimate probabilities for the same positions\n",
    "    #probabilities_estimated = [estimate(p[people_order], k) for k in positions_simulated]\n",
    "\n",
    "    # Calculate relative errors\n",
    "    rel_errors = [abs(est - sim) / sim for est, sim in zip(probabilities_estimated, probabilities_simulated)]\n",
    "\n",
    "    # Create plots\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot probabilities\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(positions_simulated, probabilities_simulated, label='Simulated')\n",
    "    plt.plot(positions_simulated, probabilities_estimated, label='Estimated', linestyle='--')\n",
    "    plt.xlabel(f\"Position (k), p={p[people_order]}\")\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Probabilities: Simulated vs. Estimated')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot relative errors\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(positions_simulated, rel_errors, label='Rel. Error', color='red')\n",
    "    plt.xlabel(f\"Position (k), p={p[people_order]}\")\n",
    "    plt.ylabel('Relative Error')\n",
    "    plt.title('Relative Error: Simulated vs. Estimated')\n",
    "    plt.legend()\n",
    "\n",
    "    # Display plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Prediction Data( the data was generated by simulation from another ipynb file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predict_data_file.pkl', 'rb') as file:\n",
    "    dic_predict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_weighted_sums(data, mu, sigma):\n",
    "    \"\"\"\n",
    "    This function calculates the gaussian weighted sum for elements in the data\n",
    "    less than and greater than the mean (mu).\n",
    "\n",
    "    Parameters:\n",
    "    data (list of float): The list of data points.\n",
    "    mu (float): The mean of the gaussian distribution.\n",
    "    sigma (float): The standard deviation of the gaussian distribution.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (sum_less_than_mu, sum_greater_than_mu)\n",
    "    \"\"\"\n",
    "    sum_less_than_mu = 0\n",
    "    sum_near_mu = 0\n",
    "    sum_greater_than_mu = 0\n",
    "    \n",
    "\n",
    "    # Calculate the gaussian weighted sum\n",
    "    for p in data:\n",
    "        weight = norm.pdf(p, mu, sigma)\n",
    "        if p < mu - 0.5*sigma:\n",
    "            sum_less_than_mu += weight\n",
    "        elif p > mu + 0.5*sigma:\n",
    "            sum_greater_than_mu += weight\n",
    "        else:\n",
    "            sum_near_mu += weight\n",
    "    sum_less_than_mu /= len(data)\n",
    "    sum_near_mu /= len(data)\n",
    "    sum_greater_than_mu /= len(data)\n",
    "    return [sum_less_than_mu, sum_near_mu, sum_greater_than_mu]\n",
    "def square_weighted_sums(data, mu, power):\n",
    "    \"\"\"\n",
    "    This function calculates the gaussian weighted sum for elements in the data\n",
    "    less than and greater than the mean (mu).\n",
    "\n",
    "    Parameters:\n",
    "    data (list of float): The list of data points.\n",
    "    mu (float): The mean of the gaussian distribution.\n",
    "    sigma (float): The standard deviation of the gaussian distribution.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (sum_less_than_mu, sum_greater_than_mu)\n",
    "    \"\"\"\n",
    "    sum_less_than_mu = 0\n",
    "    sum_greater_than_mu = 0\n",
    "\n",
    "    # Calculate the gaussian weighted sum\n",
    "    for p in data:\n",
    "        weight = (abs(p-mu))**power\n",
    "        if p < mu:\n",
    "            sum_less_than_mu += weight\n",
    "        elif p > mu:\n",
    "            sum_greater_than_mu += weight\n",
    "    sum_less_than_mu /= len(data)\n",
    "    sum_greater_than_mu /= len(data)\n",
    "    return [sum_less_than_mu, sum_greater_than_mu]\n",
    "\n",
    "def generate_input_data(data,p):\n",
    "    # 59\n",
    "    output=[]\n",
    "    sigma_list = [p/20,p/15,p/10,p/8,p/4,p/2,p,2*p,4*p,8*p,16*p,32*p,0.001,0.002,0.004,0.008,0.016,0.032,0.064,0.128,0.256]\n",
    "    power_list = [1/20,1/15,1/10,1/8,1/4,1/2,1,1.1,1.2,1,4,1.8,2,2.4,3,3.5,4]\n",
    "    for sigma in sigma_list:\n",
    "        output+=gaussian_weighted_sums(data,0,sigma)\n",
    "    for power in power_list:\n",
    "        output+=square_weighted_sums(data,0,power)\n",
    "    return output#,len(sigma_list)*3+len(power_list)*2\n",
    "def count_ranges(data, n=100):\n",
    "    counts = [0] * n\n",
    "\n",
    "    for number in data:\n",
    "        if 0 <= number < 1:\n",
    "            index = int(number * n)\n",
    "            counts[index] += 1\n",
    "\n",
    "    # 将 counts 转换为浮点数数组\n",
    "    counts = np.array(counts, dtype=float)\n",
    "\n",
    "    # 确保不会发生除以零的错误\n",
    "    total = sum(counts)\n",
    "    if total != 0:\n",
    "        counts /= total\n",
    "\n",
    "    return list(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define custom layer for ln(x) operation\n",
    "class CustomLogLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Applying ln(x) for x > 0, else -10\n",
    "        return torch.where(x > 0, torch.log(x), torch.full_like(x, -10.0))\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        # Define layers for path 1\n",
    "        self.dense1_path1 = nn.Linear(107, 500)\n",
    "        self.custom_log_layer = CustomLogLayer()\n",
    "        #self.combine_dense=nn.Linear(3103, 3000)\n",
    "        self.dense2_path1 = nn.Linear(500, 500)\n",
    "        #self.dense3_path1 = nn.Linear(1500, 1500)\n",
    "        #self.combine_dense=nn.Linear(3000, 3000)\n",
    "\n",
    "        # Define layer for path 2\n",
    "        self.dense_path2 = nn.Linear(107, 500)\n",
    "\n",
    "        # Define layers after concatenation\n",
    "        self.dense1_combined = nn.Linear(1000, 2000)\n",
    "        self.dense2_combined = nn.Linear(2000, 1000)\n",
    "        self.dense3_combined = nn.Linear(1000, 500)\n",
    "        self.dense4_combined = nn.Linear(500, 100)\n",
    "        self.dense5_combined = nn.Linear(100, 1)\n",
    "        # self.dense6_combined = nn.Linear(24, 50)\n",
    "        # self.dense7_combined = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Path 1\n",
    "        x1 = F.relu(self.dense1_path1(x))\n",
    "        x1 = self.custom_log_layer(x1)\n",
    "        #x1 = torch.cat([x1, x], dim=0)\n",
    "        #x1 = self.combine_dense(x1)\n",
    "        x1 = torch.exp(self.dense2_path1(x1))\n",
    "        x1 = torch.pow(x1, 1/4)\n",
    "\n",
    "        \n",
    "\n",
    "        # Path 2\n",
    "        x2 = F.relu(self.dense_path2(x))\n",
    "        #x2 = x\n",
    "\n",
    "\n",
    "        # Concatenate\n",
    "        combined = torch.cat([x1, x2], dim=0)\n",
    "\n",
    "        # Path 3\n",
    "        x3=x[:4]\n",
    "\n",
    "        # Further layers\n",
    "        combined = F.relu(self.dense1_combined(combined))\n",
    "        combined = F.relu(self.dense2_combined(combined))\n",
    "        combined = F.relu(self.dense3_combined(combined))\n",
    "        combined = F.relu(self.dense4_combined(combined))\n",
    "        output = self.dense5_combined(combined)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(predicted, target, weight):\n",
    "    # 计算每个元素的平方差\n",
    "    squared_diff = (predicted - target) ** 2\n",
    "\n",
    "    # 为第三个元素赋予更大的权重\n",
    "    #squared_diff[2] *= weight\n",
    "\n",
    "    return squared_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "model = MyNetwork().double()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dic_predict,epochs):\n",
    "    key_list=list(dic_predict.keys())\n",
    "    key_list_len=len(key_list)\n",
    "    loss_sum = 0\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        random_choice = random.randint(0, key_list_len - 1)\n",
    "        key = key_list[random_choice]\n",
    "        N, L, p_list = key\n",
    "        result = dic_predict[key]\n",
    "\n",
    "\n",
    "        for time in range(1):\n",
    "            random_choice_people = random.randint(0, N - 1)\n",
    "            p = p_list[random_choice_people]\n",
    "            data_list = generate_input_data(p_list, p)\n",
    "            lin_list=count_ranges(p_list, n=100)\n",
    "            mu=np.mean(p_list)\n",
    "            data_list=[1/p]+[p]+[L/N]+[N/L-1]+[N*p/L]+[L/N/p]+[mu]+[1/mu]+[mu*N/L]+[L/N/mu]+data_list\n",
    "            #print(result[random_choice_people])\n",
    "            data_tensor = torch.tensor(data_list, dtype=torch.double).to(device)\n",
    "            target_tensor = torch.tensor(result[random_choice_people][2], dtype=torch.double).to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            predicted = model(data_tensor)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = weighted_mse_loss(predicted, target_tensor, weight)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 累加损失\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        # 每100个epoch计算平均损失并重置累加器\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            average_loss = loss_sum / 100\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Average Loss: {average_loss}')\n",
    "            loss_sum = 0\n",
    "\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train(dic_predict,4500)\n",
    "torch.save(model.state_dict(), 'model_sample2.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
